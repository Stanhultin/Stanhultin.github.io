
---
output: 
  html_document:
  pdf_document: default
  word_document: default
title: "Assignment 12: Predictive Modeling - Part 3"
---

1. Install the package `mlbench` and use the follows to import the data

```{r}
library(mlbench)
data(PimaIndiansDiabetes)
df <- PimaIndiansDiabetes
```

- Set seed to be 2020. 
- The target variable is `diabetes`
- Partition the data into 80% training and 20% testing.  
```{r}
library(caret)
set.seed(2020)
splitIndex <- createDataPartition(df$diabetes, p = .80, 
                                  list = FALSE)
df_train <- df[ splitIndex,]
df_test <- df[-splitIndex,]
```

-------

2. Use cross-validation of 30 folds to tune random forest (method='rf').  What is the `mtry` value that produces the greatest accuracy?

The mtry value of 1 produces the greatest accuracy, followed by a value of 3.
```{r}
tuneGrid = expand.grid(mtry = 1:4)
trControl = trainControl(method = "cv",
                         number = 10)
forest_cv <- train(diabetes~., data=df_train, 
                                method = "rf", 
                                trControl = trControl,
                                tuneGrid = tuneGrid)
```

```{r}
plot(forest_cv)
```
 
-------

3. Use cross-validation with of 30 folds to tune random forest (method='ranger').  What are the parameters that produce the greatest accuracy?

The parameters that produce the greatest accuracy are the extratrees. It is more accurate than gini. 4is the most accurate in the extratree model.
```{r}
trControl = trainControl(method = "cv",
                         number = 10)
tuneGrid = expand.grid(mtry = 2:4,
                       splitrule = c('gini', 'extratrees'),
                       min.node.size = c(1:10))
ranger_cv <- train(diabetes~., data=df_train, 
                    method = "ranger", 
                    trControl = trControl,
                    tuneGrid = tuneGrid)
plot(ranger_cv)
```


-------

4. Go to https://topepo.github.io/caret/available-models.html and pick a classification model.  Tune the classification model using cross-validation of 30 folds. 
```{r}
getModelInfo('adaboost')$adaboost$parameters
```
```{r}
trControl = trainControl(method = "cv",
                         number = 10)
adaboost_cv <- train(diabetes~., data=df_train, 
                    method = "adaboost",
                     trControl = trControl)
                  
plot(adaboost_cv)
```




5. Compare the three models in question 2, 3, and 4 to select the final model.  Evaluate the accuracy of the final model on the test data. 
```{r}
results <- resamples(list(forest = forest_cv,
                          ranger = ranger_cv,
                          adaboost= adaboost_cv))
bwplot(results)
```

